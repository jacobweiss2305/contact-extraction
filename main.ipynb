{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from kor.extraction import create_extraction_chain\n",
    "from kor.nodes import Object, Text, Number\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from kor import extract_from_documents, from_pydantic, create_extraction_chain\n",
    "\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from utils import load_conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = load_conversation('emails/sample_4.txt')\n",
    "doc = Document(page_content=conversation)\n",
    "split_docs = RecursiveCharacterTextSplitter().split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"emails/sample_4.txt\", encoding=\"utf-8\") as f:\n",
    "    conversation = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt = f\"\"\"\n",
    "\n",
    "Your goal is to extract structured information from the email chain below that matches the form described below. \n",
    "When extracting information, please make sure it matches the type information exactly. Do not add any attributes that do not appear in the schema shown below.\n",
    "\n",
    "```TypeScript\n",
    "\n",
    "personal_info: Array<{{ // Contact information for a person\n",
    " first_name: string // The first name of the person\n",
    " last_name: string // The last name of the person\n",
    " job_title: string // The job title of the person\n",
    " company_name: string // The company name the person works for\n",
    " mobile_number: string // The mobile or direct number of the person\n",
    " desk_number: string // The desk or office number of the person\n",
    " business_website: string // The business website of the company that the person works for\n",
    " email: string // The email of the person\n",
    " address: string // The address of the company the person works for\n",
    "}}>\n",
    "\n",
    "Please output the extracted information in JSON format.\n",
    "Do NOT add any clarifying information. Output MUST follow the schema above. Do NOT add any additional columns that do not appear in the schema.\n",
    "\n",
    "EMAIL CHAIN:\n",
    "{conversation}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "output = llm.predict(prompt,)\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "pd.DataFrame(json.loads(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a world class algorithm for extracting information in structured formats.\n",
      "Human: Use the given format to extract information from the following input:\n",
      "Human: Sally is 13\n",
      "Human: Tips: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Person(name='Sally', age=13, fav_food='Unknown')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    first_name: str\n",
    "    last_name: str\n",
    "    job_title: str\n",
    "    company_name: str\n",
    "    mobile_number: str\n",
    "    desk_number: str\n",
    "    business_website: str\n",
    "    email: str\n",
    "    address: str\n",
    "\n",
    "class PersonalInfo(BaseModel):\n",
    "    personal_info: List[ContactInfo]\n",
    "\n",
    "\n",
    "# If we pass in a model explicitly, we need to make sure it supports the OpenAI function-calling API.\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "prompt_msgs = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a world class algorithm for extracting information in structured formats.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Use the given format to extract information from the following input:\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    HumanMessage(content=\"Tips: Make sure to answer in the correct format\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate(messages=prompt_msgs)\n",
    "\n",
    "chain = create_structured_output_chain(PersonalInfo, llm, prompt, verbose=True)\n",
    "chain.run(conversation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
